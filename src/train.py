import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import time
import os
import matplotlib.pyplot as plt
from model import get_model
from dataset import get_dataloaders


def train_model(data_dir="data", epochs=5, batch_size=16, lr=0.001):
    """ê°„ë‹¨í•˜ê³  ë¹ ë¥¸ ëª¨ë¸ í•™ìŠµ + ì‹œê°í™”"""
    
    print("ğŸš€ ë§ˆìŠ¤í¬ ì°©ìš© ê°ì§€ê¸° í•™ìŠµ ì‹œì‘!")
    print("=" * 50)
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ’» ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    train_loader, val_loader = get_dataloaders(data_dir, batch_size)
    
    # ëª¨ë¸ ìƒì„±
    model = get_model()
    model = model.to(device)
    
    # ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    
    # í•™ìŠµ ê¸°ë¡ (ì‹œê°í™”ìš©)
    train_losses = []
    train_accuracies = []
    val_accuracies = []
    best_val_acc = 0.0
    
    print(f"ğŸ“š ë°°ì¹˜ í¬ê¸°: {batch_size}, í•™ìŠµë¥ : {lr}, ì—í¬í¬: {epochs}")
    print("=" * 50)
    
    for epoch in range(epochs):
        start_time = time.time()
        
        # í›ˆë ¨ ë‹¨ê³„
        model.train()
        running_loss = 0.0
        train_correct = 0
        train_total = 0
        
        print(f"ğŸ“– Epoch {epoch+1}/{epochs} - í›ˆë ¨ ì¤‘...")
        
        for batch_idx, (images, labels) in enumerate(train_loader):
            images, labels = images.to(device), labels.to(device)
            
            # ìˆœì „íŒŒ
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            # ì—­ì „íŒŒ
            loss.backward()
            optimizer.step()
            
            # í†µê³„
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += labels.size(0)
            train_correct += predicted.eq(labels).sum().item()
            
            # ì§„í–‰ìƒí™© ì¶œë ¥ (10ë°°ì¹˜ë§ˆë‹¤)
            if batch_idx % 10 == 0:
                print(f"   ë°°ì¹˜ {batch_idx}/{len(train_loader)}, ì†ì‹¤: {loss.item():.4f}")
        
        # í›ˆë ¨ ê²°ê³¼
        train_loss = running_loss / len(train_loader)
        train_acc = 100. * train_correct / train_total
        
        # ê²€ì¦ ë‹¨ê³„
        model.eval()
        val_correct = 0
        val_total = 0
        
        print(f"ğŸ” ê²€ì¦ ì¤‘...")
        
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                _, predicted = outputs.max(1)
                val_total += labels.size(0)
                val_correct += predicted.eq(labels).sum().item()
        
        val_acc = 100. * val_correct / val_total
        
        # ê²°ê³¼ ì €ì¥ (ì‹œê°í™”ìš©)
        train_losses.append(train_loss)
        train_accuracies.append(train_acc)
        val_accuracies.append(val_acc)
        
        # ìµœê³  ëª¨ë¸ ì €ì¥
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            os.makedirs('models', exist_ok=True)
            torch.save(model.state_dict(), 'models/best_model.pth')
            print(f"ğŸ’¾ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥! (ì •í™•ë„: {val_acc:.2f}%)")
        
        # ì—í¬í¬ ê²°ê³¼ ì¶œë ¥
        epoch_time = time.time() - start_time
        print(f"âœ… Epoch {epoch+1} ì™„ë£Œ ({epoch_time:.1f}ì´ˆ)")
        print(f"   í›ˆë ¨ ì†ì‹¤: {train_loss:.4f}, í›ˆë ¨ ì •í™•ë„: {train_acc:.2f}%")
        print(f"   ê²€ì¦ ì •í™•ë„: {val_acc:.2f}%")
        print("-" * 30)
    
    print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
    print(f"ğŸ† ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_acc:.2f}%")
    print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: models/best_model.pth")
    
    # ğŸ“Š í•™ìŠµ ê³¡ì„  ì‹œê°í™” ìƒì„±
    create_training_plots(train_losses, train_accuracies, val_accuracies, epochs)
    
    return model, train_losses, train_accuracies, val_accuracies


def create_training_plots(train_losses, train_accuracies, val_accuracies, epochs):
    """í•™ìŠµ ê³¡ì„  ì‹œê°í™” ìƒì„±"""
    
    print("ğŸ“ˆ í•™ìŠµ ê³¡ì„  ì‹œê°í™” ìƒì„± ì¤‘...")
    os.makedirs('results', exist_ok=True)
    
    # í•œê¸€ í°íŠ¸ ì„¤ì • (Windows)
    plt.rcParams['font.family'] = 'Malgun Gothic'
    plt.rcParams['axes.unicode_minus'] = False
    
    # ê·¸ë˜í”„ ìƒì„± (2x1 ë ˆì´ì•„ì›ƒ)
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))
    epochs_range = range(1, epochs + 1)
    
    # 1. ì†ì‹¤ ê·¸ë˜í”„
    ax1.plot(epochs_range, train_losses, 'b-', label='í›ˆë ¨ ì†ì‹¤', linewidth=2)
    ax1.set_title('ğŸ”„ ëª¨ë¸ í•™ìŠµ ì†ì‹¤ ë³€í™”', fontsize=14, fontweight='bold')
    ax1.set_xlabel('ì—í¬í¬')
    ax1.set_ylabel('ì†ì‹¤ (Loss)')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. ì •í™•ë„ ê·¸ë˜í”„
    ax2.plot(epochs_range, train_accuracies, 'g-', label='í›ˆë ¨ ì •í™•ë„', linewidth=2)
    ax2.plot(epochs_range, val_accuracies, 'r-', label='ê²€ì¦ ì •í™•ë„', linewidth=2)
    ax2.set_title('ğŸ“Š ëª¨ë¸ ì •í™•ë„ ë³€í™”', fontsize=14, fontweight='bold')
    ax2.set_xlabel('ì—í¬í¬')
    ax2.set_ylabel('ì •í™•ë„ (%)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # ìµœì¢… ê²°ê³¼ í…ìŠ¤íŠ¸ ì¶”ê°€
    final_train_acc = train_accuracies[-1]
    final_val_acc = val_accuracies[-1]
    ax2.text(0.02, 0.98, f'ìµœì¢… í›ˆë ¨ ì •í™•ë„: {final_train_acc:.1f}%\nìµœì¢… ê²€ì¦ ì •í™•ë„: {final_val_acc:.1f}%', 
             transform=ax2.transAxes, fontsize=10,
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
    
    plt.tight_layout()
    
    # ì €ì¥
    save_path = 'results/training_curves.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… í•™ìŠµ ê³¡ì„  ì €ì¥ ì™„ë£Œ: {save_path}")
    return save_path


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ë¹ ë¥¸ í•™ìŠµì„ ìœ„í•œ ì„¤ì •
    EPOCHS = 3  # ì—í¬í¬ ìˆ˜ ì ê²Œ
    BATCH_SIZE = 32  # ë°°ì¹˜ í¬ê¸° í¬ê²Œ (ë¹ ë¥¸ í•™ìŠµ)
    LEARNING_RATE = 0.001
    
    print("ğŸ˜· ë§ˆìŠ¤í¬ ì°©ìš© ê°ì§€ê¸° - ë¹ ë¥¸ í•™ìŠµ ë²„ì „")
    print(f"âš¡ ì„¤ì •: {EPOCHS} ì—í¬í¬, ë°°ì¹˜ {BATCH_SIZE}")
    
    # ë°ì´í„° í´ë” í™•ì¸
    if not os.path.exists("data"):
        print("âŒ ì˜¤ë¥˜: 'data' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        print("ğŸ’¡ data/with_mask/ ì™€ data/without_mask/ í´ë”ë¥¼ ìƒì„±í•˜ê³  ì´ë¯¸ì§€ë¥¼ ë„£ì–´ì£¼ì„¸ìš”.")
        return
    
    try:
        # ëª¨ë¸ í•™ìŠµ
        model, losses, train_accs, val_accs = train_model(
            data_dir="data",
            epochs=EPOCHS,
            batch_size=BATCH_SIZE,
            lr=LEARNING_RATE
        )
        
        # ê°„ë‹¨í•œ ê²°ê³¼ ìš”ì•½
        print("\nğŸ“Š í•™ìŠµ ê²°ê³¼ ìš”ì•½:")
        print(f"   ìµœì¢… í›ˆë ¨ ì†ì‹¤: {losses[-1]:.4f}")
        print(f"   ìµœì¢… í›ˆë ¨ ì •í™•ë„: {train_accs[-1]:.2f}%")
        print(f"   ìµœì¢… ê²€ì¦ ì •í™•ë„: {val_accs[-1]:.2f}%")
        print(f"ğŸ“ˆ ì‹œê°í™” ê²°ê³¼ëŠ” results/ í´ë”ì—ì„œ í™•ì¸í•˜ì„¸ìš”!")
        
    except Exception as e:
        print(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ğŸ’¡ ë°ì´í„° ê²½ë¡œì™€ ì´ë¯¸ì§€ íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    main()
